{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f2yu8Dwm5_vW"
   },
   "source": [
    "#**Introduction**\n",
    "-------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "R47aexX_7B9q"
   },
   "source": [
    "To retain treasured subscribers in state-of-the-art aggressive market, telecom organizations have to be able to are expecting purchaser attrition. This be counted is tackled by way of this venture utilising the data dataset from Kaggle. Customer facts and attrition indicators are contained on this huge dataset. The study will assemble a machine learning pipeline that predicts the likelihood of turnover primarily based on an evaluation of consumer behaviour to be able to fight attrition. Potential benefits of identifying at-hazard consumers include the implementation of centered retention strategies and a discount in customer turnover.\n",
    "\n",
    "** Dataset Link: https://www.kaggle.com/datasets/blastchar/telco-customer-churn **\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BAt1EDEx6Gw9"
   },
   "source": [
    "#**Problem Statement**\n",
    "-----\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rKb4KkcR7p13"
   },
   "source": [
    "Customer churn, the loss of subscribers, is a major financial concern for telecom companies. The motive of this endeavour is to create a churn prediction version in order to deter clients from leaving.  Prolonged patron attrition costs considerably hinder growth and sales, necessitating ongoing tasks to gather sparkling clients.  As stated by Zhang et al. (2022), should the attrition prediction machine prove to be powerful, the employer might also favor to put into effect centered retention strategies, which includes stronger provider plans or distinct reductions, to draw and maintain purchasers who are at danger.  This problem may be characterized as a binary classification endeavour that uses consumer information, which include demographics, invoicing records, and provider utilization styles.  The predictive model determines whether a user will remain churn (1) or remain a subscriber (0).  This in the end fosters customer loyalty and pride through allowing the agency to allocate assets strategically and personalize approaches to client retention."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_ZjWnpzc6ROL"
   },
   "source": [
    "#**Data Exploration**\n",
    "----\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bm5x14sL_MXt",
    "outputId": "c5aa60a2-4e90-4c7c-d9cb-65a5db8c6180"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 7043 entries, 0 to 7042\n",
      "Data columns (total 21 columns):\n",
      " #   Column            Non-Null Count  Dtype  \n",
      "---  ------            --------------  -----  \n",
      " 0   customerID        7043 non-null   object \n",
      " 1   gender            7043 non-null   object \n",
      " 2   SeniorCitizen     7043 non-null   int64  \n",
      " 3   Partner           7043 non-null   object \n",
      " 4   Dependents        7043 non-null   object \n",
      " 5   tenure            7043 non-null   int64  \n",
      " 6   PhoneService      7043 non-null   object \n",
      " 7   MultipleLines     7043 non-null   object \n",
      " 8   InternetService   7043 non-null   object \n",
      " 9   OnlineSecurity    7043 non-null   object \n",
      " 10  OnlineBackup      7043 non-null   object \n",
      " 11  DeviceProtection  7043 non-null   object \n",
      " 12  TechSupport       7043 non-null   object \n",
      " 13  StreamingTV       7043 non-null   object \n",
      " 14  StreamingMovies   7043 non-null   object \n",
      " 15  Contract          7043 non-null   object \n",
      " 16  PaperlessBilling  7043 non-null   object \n",
      " 17  PaymentMethod     7043 non-null   object \n",
      " 18  MonthlyCharges    7043 non-null   float64\n",
      " 19  TotalCharges      7043 non-null   object \n",
      " 20  Churn             7043 non-null   object \n",
      "dtypes: float64(1), int64(2), object(18)\n",
      "memory usage: 1.1+ MB\n",
      "None\n",
      "       SeniorCitizen       tenure  MonthlyCharges\n",
      "count    7043.000000  7043.000000     7043.000000\n",
      "mean        0.162147    32.371149       64.761692\n",
      "std         0.368612    24.559481       30.090047\n",
      "min         0.000000     0.000000       18.250000\n",
      "25%         0.000000     9.000000       35.500000\n",
      "50%         0.000000    29.000000       70.350000\n",
      "75%         0.000000    55.000000       89.850000\n",
      "max         1.000000    72.000000      118.750000\n",
      "   customerID  gender  SeniorCitizen Partner Dependents  tenure PhoneService  \\\n",
      "0  7590-VHVEG  Female              0     Yes         No       1           No   \n",
      "1  5575-GNVDE    Male              0      No         No      34          Yes   \n",
      "2  3668-QPYBK    Male              0      No         No       2          Yes   \n",
      "3  7795-CFOCW    Male              0      No         No      45           No   \n",
      "4  9237-HQITU  Female              0      No         No       2          Yes   \n",
      "\n",
      "      MultipleLines InternetService OnlineSecurity  ... DeviceProtection  \\\n",
      "0  No phone service             DSL             No  ...               No   \n",
      "1                No             DSL            Yes  ...              Yes   \n",
      "2                No             DSL            Yes  ...               No   \n",
      "3  No phone service             DSL            Yes  ...              Yes   \n",
      "4                No     Fiber optic             No  ...               No   \n",
      "\n",
      "  TechSupport StreamingTV StreamingMovies        Contract PaperlessBilling  \\\n",
      "0          No          No              No  Month-to-month              Yes   \n",
      "1          No          No              No        One year               No   \n",
      "2          No          No              No  Month-to-month              Yes   \n",
      "3         Yes          No              No        One year               No   \n",
      "4          No          No              No  Month-to-month              Yes   \n",
      "\n",
      "               PaymentMethod MonthlyCharges  TotalCharges Churn  \n",
      "0           Electronic check          29.85         29.85    No  \n",
      "1               Mailed check          56.95        1889.5    No  \n",
      "2               Mailed check          53.85        108.15   Yes  \n",
      "3  Bank transfer (automatic)          42.30       1840.75    No  \n",
      "4           Electronic check          70.70        151.65   Yes  \n",
      "\n",
      "[5 rows x 21 columns]\n",
      "customerID          0\n",
      "gender              0\n",
      "SeniorCitizen       0\n",
      "Partner             0\n",
      "Dependents          0\n",
      "tenure              0\n",
      "PhoneService        0\n",
      "MultipleLines       0\n",
      "InternetService     0\n",
      "OnlineSecurity      0\n",
      "OnlineBackup        0\n",
      "DeviceProtection    0\n",
      "TechSupport         0\n",
      "StreamingTV         0\n",
      "StreamingMovies     0\n",
      "Contract            0\n",
      "PaperlessBilling    0\n",
      "PaymentMethod       0\n",
      "MonthlyCharges      0\n",
      "TotalCharges        0\n",
      "Churn               0\n",
      "dtype: int64\n",
      "Outliers in SeniorCitizen: 1142\n",
      "Outliers in tenure: 0\n",
      "Outliers in MonthlyCharges: 0\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "# Load the dataset\n",
    "data = pd.read_csv('data.csv')\n",
    "\n",
    "# Describe the dataset\n",
    "print(data.info())\n",
    "print(data.describe())\n",
    "print(data.head())\n",
    "\n",
    "# Check for missing values\n",
    "print(data.isnull().sum())\n",
    "\n",
    "# Check for outliers (assuming numerical columns)\n",
    "numerical_cols = data.select_dtypes(include=['int64', 'float64']).columns\n",
    "for col in numerical_cols:\n",
    "    # Calculate the IQR (Interquartile Range)\n",
    "    Q1 = data[col].quantile(0.25)\n",
    "    Q3 = data[col].quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "\n",
    "    # Check for outliers using the IQR method\n",
    "    lower_bound = Q1 - 1.5 * IQR\n",
    "    upper_bound = Q3 + 1.5 * IQR\n",
    "    outliers = ((data[col] < lower_bound) | (data[col] > upper_bound)).sum()\n",
    "    print(f\"Outliers in {col}: {outliers}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ed-YHxvp8CgE"
   },
   "source": [
    "The chosen dataset, data, is to be had to be used on Kaggle and accommodates consumer facts this is pertinent to the churn estimation of a telecommunications business enterprise. Important elements are likely to include demographics, subscription carrier plans (e.g., internet, smartphone), usage patterns (e.g., information usage, call duration), and invoice records (e.G., tenure, price technique). Anonymity detection in use pattern anomalies, evaluation of churn label distribution (probably unequal inside the case of a extra percentage of non-churned clients), and search for absent variables are all additives of information investigation. Data cleaning can also involve the removal of items which have a giant fee of missing values or the meticulous inputting of absent records (Nalatissifa and Pardede, 2021).  Depending on the diploma of imbalance, sampling techniques inclusive of oversampling or undersampling the minority magnificence (churned clients) can be required to make sure that the version learns efficiently.  In order to assess the performance of the version, class metrics consisting of accuracy, precision, recall, and the F1 rating can be hired.  Accuracy assesses average correctness, whereas precision and recall are concerned with precisely identifying non-churned and churned consumers, respectively.  The F1 score offers a rational assessment of each."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rzCA5n4s6Vkj"
   },
   "source": [
    "#**Data Preprocessing & Feature Engineering**\n",
    "-----\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "ExZggiTP_pOA"
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.impute import SimpleImputer\n",
    "# Convert 'TotalCharges' to numeric, replacing non-numeric values with NaN\n",
    "data['TotalCharges'] = pd.to_numeric(data['TotalCharges'], errors='coerce')\n",
    "\n",
    "# Handle missing values using the mean strategy\n",
    "imputer = SimpleImputer(strategy='mean')\n",
    "data['TotalCharges'] = imputer.fit_transform(data[['TotalCharges']])\n",
    "\n",
    "# Encode categorical features\n",
    "encoder = OneHotEncoder(drop='first')\n",
    "encoded_features = encoder.fit_transform(data[['gender', 'Partner', 'Dependents', 'PhoneService', 'MultipleLines',\n",
    "                                                'InternetService', 'OnlineSecurity', 'OnlineBackup',\n",
    "                                                'DeviceProtection', 'TechSupport', 'StreamingTV',\n",
    "                                                'StreamingMovies', 'Contract', 'PaperlessBilling',\n",
    "                                                'PaymentMethod']]).toarray()\n",
    "encoded_df = pd.DataFrame(encoded_features, columns=encoder.get_feature_names_out(['gender', 'Partner', 'Dependents',\n",
    "                                                                              'PhoneService', 'MultipleLines',\n",
    "                                                                              'InternetService', 'OnlineSecurity',\n",
    "                                                                              'OnlineBackup', 'DeviceProtection',\n",
    "                                                                              'TechSupport', 'StreamingTV',\n",
    "                                                                              'StreamingMovies', 'Contract',\n",
    "                                                                              'PaperlessBilling', 'PaymentMethod']))\n",
    "\n",
    "# Scale numerical features\n",
    "scaler = StandardScaler()\n",
    "scaled_features = scaler.fit_transform(data[['tenure', 'MonthlyCharges', 'TotalCharges']])\n",
    "scaled_df = pd.DataFrame(scaled_features, columns=['tenure', 'MonthlyCharges', 'TotalCharges'])\n",
    "\n",
    "# Combine the encoded and scaled features with the original dataset\n",
    "data_processed = pd.concat([data.drop(columns=['gender', 'Partner', 'Dependents', 'PhoneService', 'MultipleLines',\n",
    "                                                'InternetService', 'OnlineSecurity', 'OnlineBackup',\n",
    "                                                'DeviceProtection', 'TechSupport', 'StreamingTV',\n",
    "                                                'StreamingMovies', 'Contract', 'PaperlessBilling',\n",
    "                                                'PaymentMethod', 'tenure', 'MonthlyCharges', 'TotalCharges']),\n",
    "                             encoded_df, scaled_df], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "REhthHkr-Lv0"
   },
   "source": [
    "Data preparation will be crucial for building a robust model. The study will  address missing values strategically: imputing numerical features with appropriate methods (mean, median) and potentially removing entries with too many missing values. Additionally, entries with a massive range of lacking values may be eliminated. In order to mitigate the effect of utilisation sample anomalies on the model, such styles can be limited or winorized. In order to numerically represent specific facts, together with provider plans for the model, one-hot encoding might be implemented (Momin et al. 2020). Through feature engineering, extra features may be delivered to a model to decorate its efficacy.  Deductions consisting of \"general month-to-month spending\" may be made from billing and consumption records. Additionally, classification functions for customer tenure (e.G., \"much less than 12 months,\" \"1-2 years\") should be included so that you can gain a greater comprehensive information of the movements taken by means of clients who discontinue their patronage at one of a kind tiers in their lifetimes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8o7B_joB6Z4f"
   },
   "source": [
    "#**Model Training**\n",
    "-----\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "JmvHG1w9A5sy",
    "outputId": "4de2eef4-d5f1-42b1-fcca-36b9474524b1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of X: (7043, 7072)\n",
      "Shape of y: (7043,)\n",
      "Distribution of y:\n",
      "Churn\n",
      "0    5174\n",
      "1    1869\n",
      "Name: count, dtype: int64\n",
      "Best hyperparameters: {'C': 10}\n",
      "Accuracy: 0.7863733144073811\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.79      0.96      0.87      1036\n",
      "           1       0.72      0.31      0.44       373\n",
      "\n",
      "    accuracy                           0.79      1409\n",
      "   macro avg       0.76      0.64      0.65      1409\n",
      "weighted avg       0.78      0.79      0.75      1409\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "\n",
    "# Handle missing values\n",
    "data['TotalCharges'] = pd.to_numeric(data['TotalCharges'], errors='coerce')\n",
    "data.dropna(inplace=True)\n",
    "\n",
    "# Encode categorical features\n",
    "X = data.drop('Churn', axis=1)\n",
    "X = pd.get_dummies(X, drop_first=True)\n",
    "y = data['Churn'].map({'Yes': 1, 'No': 0})\n",
    "\n",
    "# Check the shape of X and y\n",
    "print(\"Shape of X:\", X.shape)\n",
    "print(\"Shape of y:\", y.shape)\n",
    "\n",
    "# Check the distribution of the target variable\n",
    "print(\"Distribution of y:\")\n",
    "print(y.value_counts())\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Standardize the features\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# Choose and justify the algorithm (Logistic Regression)\n",
    "model = LogisticRegression()\n",
    "\n",
    "# Hyperparameter tuning\n",
    "param_grid = {'C': [0.001, 0.01, 0.1, 1, 10, 100]}  # Regularization parameter\n",
    "grid_search = GridSearchCV(model, param_grid, cv=5, scoring='accuracy')\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Get the best hyperparameters\n",
    "best_params = grid_search.best_params_\n",
    "\n",
    "# Train the model with the best hyperparameters\n",
    "best_model = LogisticRegression(**best_params)\n",
    "best_model.fit(X_train, y_train)\n",
    "\n",
    "# Predict on the test set\n",
    "y_pred = best_model.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "report = classification_report(y_test, y_pred)\n",
    "\n",
    "print(f\"Best hyperparameters: {best_params}\")\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "print(\"Classification Report:\\n\", report)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bBK6JkalD5iO"
   },
   "source": [
    "Logistic Regression for Churn Prediction:\n",
    "--------------\n",
    "Given that it is able to control the binary category hassle of attrition prediction and that the dataset is probable to include both numeric and express elements, logistic regression is an fantastic alternative. It accurately simulates the relationship that exists among attributes and binary consequences (Fujo et al. 2022). A linear combination of weighted attributes is utilised with the aid of Logistic Regression to compute the opportunity that a given facts point is a member of a specific magnificence (churned in this study case).\n",
    "\n",
    "Hyperparameter Tuning for Optimal Performance:\n",
    "----------------\n",
    "GridSearchCV will be hired to optimise the version and acquire the maximum beneficial consequences. This method employs pass-validation to assess various model configurations through the exam of a predetermined grid of hyperparameter values ('C' being the regularisation parameter on this example). In order to decide the most advantageous version, the configuration that achieves the best accuracy at the validation set can be evaluated (Mustafa et al. 2021).\n",
    "\n",
    "The provided code demonstrates several key steps:\n",
    "-----------\n",
    "* Data Preprocessing: In the method of statistics preprocessing, numerical conversion and row elimination are carried out to deal with the issue of missing values in the \"TotalCharges\" column when a couple of gadgets are absent.\n",
    "* Feature Engineering: One-warm encoding is utilised to encode express features so that the version can comprehend them.\n",
    "* Train-Test Split: The method we employ partitions the statistics into training and testing units, with an 80%/20% department as illustrated inside the technique.\n",
    "* Feature Scaling: In order to maintain a uniform scale for all capabilities, the 'StandardScaler' is hired to standardise numerical features.\n",
    "* Logistic Regression Model: A clean 'LogisticRegression' example is created.\n",
    "* Hyperparameter Tuning: A grid seek is accomplished making use of the 'GridSearchCV' characteristic, incorporating exclusive values of the regularisation parameter 'C'. Maximum accuracy turned into received by the chosen version on the validation folds.\n",
    "* Model Training: The closing version, which incorporates the most most beneficial hyperparameters, is constructed using the education set.\n",
    "* Evaluation: In order to evaluate the model's performance on test facts that it has not encountered earlier than, an assessment is conducted making use of a category record and accuracy metrics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d2g8dWm-6fAn"
   },
   "source": [
    "#**Model Assessment**\n",
    "----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qt9NsuQQKqhe",
    "outputId": "bc57d60e-ccec-4656-9325-0aad24956692"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best hyperparameters: {'classifier__C': 1}\n",
      "Accuracy: 0.8218594748048261\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "          No       0.86      0.90      0.88      1036\n",
      "         Yes       0.69      0.60      0.64       373\n",
      "\n",
      "    accuracy                           0.82      1409\n",
      "   macro avg       0.77      0.75      0.76      1409\n",
      "weighted avg       0.82      0.82      0.82      1409\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "import pandas as pd\n",
    "\n",
    "# Assuming 'data_processed' contains the preprocessed dataset\n",
    "\n",
    "# Separate features and target variable\n",
    "X = data_processed.drop('Churn', axis=1)\n",
    "y = data_processed['Churn']\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Define preprocessing steps for numeric and categorical features\n",
    "numeric_features = X.select_dtypes(include=['int64', 'float64']).columns.tolist()\n",
    "categorical_features = X.select_dtypes(include=['object']).columns.tolist()\n",
    "\n",
    "numeric_transformer = Pipeline(steps=[\n",
    "    ('scaler', StandardScaler())\n",
    "])\n",
    "\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('encoder', OneHotEncoder(handle_unknown='ignore'))\n",
    "])\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numeric_transformer, numeric_features),\n",
    "        ('cat', categorical_transformer, categorical_features)\n",
    "    ])\n",
    "\n",
    "# Define the model\n",
    "model = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('classifier', LogisticRegression())\n",
    "])\n",
    "\n",
    "# Hyperparameter tuning\n",
    "param_grid = {'classifier__C': [0.001, 0.01, 0.1, 1, 10, 100]}  # Regularization parameter\n",
    "grid_search = GridSearchCV(model, param_grid, cv=5, scoring='accuracy')\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Get the best hyperparameters for LogisticRegression\n",
    "best_params_lr = grid_search.best_params_['classifier__C']\n",
    "\n",
    "# Train the model with the best hyperparameters\n",
    "best_model = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('classifier', LogisticRegression(C=best_params_lr))\n",
    "])\n",
    "best_model.fit(X_train, y_train)\n",
    "\n",
    "# Predict on the test set\n",
    "y_pred = best_model.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "report = classification_report(y_test, y_pred)\n",
    "\n",
    "print(f\"Best hyperparameters: {best_params}\")\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "print(\"Classification Report:\\n\", report)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "x9_bjAUuLob8"
   },
   "source": [
    "The final model's performance is evaluated on unseen test data using a combination of metrics. Accuracy (achieved score) measures the overall correct predictions. However, accuracy alone might be misleading in imbalanced datasets (Wanikar et al. 2024). A categorization file incorporates data in greater element. Recall quantifies the accuracy with which the machine identifies actual-existence churn eventualities, at the same time as precision signifies the percentage of predicted expelled consumers that during reality departed. These KPIs can be evaluated to determine the accuracy with which the model forecasts patron attrition."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "u_CGzXh76jA3"
   },
   "source": [
    "#**Final Discussion**\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4Ex-TmVvMKEU"
   },
   "source": [
    "This studies investigated the construction of a gadget studying infrastructure for a telecommunications employer to forecast client attrition. The Logistic Regression version, which underwent pre-processing for each categorical and numeric capabilities, demonstrated an accuracy rating of 0.79 while implemented to the checking out facts set. The categorization document encompassed supplementary information relating accuracy and recollect. Limitations persist regardless of the version's capability of identifying clients characterized by a excessive price of attrition. It is feasible for logistic regression to miss complicated relationships amongst capabilities. Furthermore, the selected metrics may be skewed if the dataset is unbalanced between instructions (with a extra share of non-churned clients).\n",
    "\n",
    "\n",
    "The consequences of this study indicate that machine getting to know ought to probably be implemented as a safety measure towards attrition. By identifying at-risk customers, the company can implement targeted retention strategies.  These endeavours will be spearheaded with the aid of emphasising the informative attributes that the version has diagnosed, together with short tenancies and coffee monthly consumption.\n",
    "\n",
    "Based on this analysis, the study recommend:\n",
    "---------------------\n",
    "\n",
    "* Refining the Model: Consider using more problematic algorithms, together with Random Forests or Gradient Boosting Machines, which can be capable of taking pictures non-linear interactions.\n",
    "* Addressing Class Imbalance: In the occasion that the records well-knownshows an imbalance, strategies which include undersampling or oversampling the minority elegance (customers who have departed) may be carried out.\n",
    "* Model Explainability: If the very last model selected allows it (as in Random Forests, as an example), a extra complete understanding of customer attrition behaviour may be acquired via analysing the maximum extensive attributes.\n",
    "\n",
    "By iteratively improving the model and incorporating explainability techniques, the company can gain valuable insights to develop effective customer retention strategies and minimize churn rates."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2KFufKZeOu6C"
   },
   "source": [
    "# References\n",
    "------\n",
    "* Fujo, S.W., Subramanian, S. and Khder, M.A., 2022. Customer churn prediction in telecommunication industry using deep learning. Information Sciences Letters, 11(1), p.24.\n",
    "* Momin, S., Bohra, T. and Raut, P., 2020. Prediction of customer churn using machine learning. In EAI International Conference on Big Data Innovation for Sustainable Cognitive Computing: BDCC 2018 (pp. 203-212). Springer International Publishing.\n",
    "* Mustafa, N., Ling, L.S. and Razak, S.F.A., 2021. Customer churn prediction for telecommunication industry: A Malaysian Case Study. F1000Research, 10.\n",
    "* Nalatissifa, H. and Pardede, H.F., 2021. Customer decision prediction using deep neural network on telco customer churn data. Jurnal Elektronika dan Telekomunikasi, 21(2), pp.122-127.\n",
    "* Wanikar, P., Maurya, S., Vishvakarma, M., Sujatha, K., Rakesh, N., Vimal, V. and Shelke, N., 2024. Telco Customer Churn Prediction Using ML Models. International Journal of Intelligent Systems and Applications in Engineering, 12(2), pp.644-653.\n",
    "* Zhang, T., Moro, S. and Ramos, R.F., 2022. A data-driven approach to improve customer churn prediction based on telecom customer segmentation. Future Internet, 14(3), p.94.\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
